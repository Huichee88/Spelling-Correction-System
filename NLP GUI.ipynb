{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob\n",
    "!pip install --upgrade spacy\n",
    "!pip install wordsegment\n",
    "!pip install jaro-winkler\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import words, names, wordnet, brown, gutenberg, stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from wordsegment import load, segment\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE, Laplace, AbsoluteDiscountingInterpolated, KneserNeyInterpolated\n",
    "from nltk.text import Text\n",
    "\n",
    "from nltk.metrics.distance import edit_distance\n",
    "import jaro\n",
    "from datetime import datetime \n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "import joblib \n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tkinter as tk\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('names')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('brown')\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class spelling_correction():\n",
    "\n",
    "    def __init__(self, distance_type, language_model_type=1, ngram=2, weight=0.5, threshold=1.e-01):\n",
    "\n",
    "        super(spelling_correction, self).__init__()\n",
    "        \n",
    "        # Load dictionary, corpus, name\n",
    "        self.dictionary = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\Models\\dictionary.joblib')\n",
    "        self.wwn = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\Models\\wwn.joblib')\n",
    "        self.named_entity = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\name (4).joblib')\n",
    "\n",
    "        # Lists to store words\n",
    "        #self.error = []\n",
    "        #self.filtered_list = []\n",
    "\n",
    "        self.confusion_sets_common = None\n",
    "        self.confusion_sets_synonym = None\n",
    "        self.distance_type = distance_type\n",
    "\n",
    "        # Load confusion sets\n",
    "        self.confusion_sets_1 = pd.read_excel(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\confusion sets.xlsx', sheet_name = \"Conso (crafted)\")\n",
    "        self.confusion_sets_2 = pd.read_csv(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\confusion_sets_2.csv', low_memory = False)\n",
    "        self.confusion_sets_1 = self.confusion_sets_1.astype(\"string\")\n",
    "        self.confusion_sets_2 = self.confusion_sets_2.astype(\"string\")\n",
    "\n",
    "        # Load ngram\n",
    "        self.laplace_bigram = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\Models\\laplace_bigram.joblib')\n",
    "        self.laplace_trigram = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\Models\\laplace_trigram.joblib')\n",
    "        self.absolute_bigram = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\Models\\absolute_bigram.joblib')\n",
    "        self.absolute_trigram = absolute_trigram = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\Models\\absolute_trigram.joblib')\n",
    "        self.kneser_bigram = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\Models\\kneser_bigram.joblib')\n",
    "        self.kneser_trigram = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\Models\\kneser_trigram.joblib')\n",
    "        self.language_model_type = language_model_type\n",
    "        self.n_gram = ngram\n",
    "\n",
    "        # Formula of noisy channel\n",
    "        self.num_deletion = pd.read_csv(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\deletion_cm.csv')\n",
    "        self.num_insertion = pd.read_csv(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\insertion_cm.csv')\n",
    "        self.num_substitution = pd.read_csv(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\substitution_cm.csv')\n",
    "        self.num_transposition = pd.read_csv(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\transposition_cm.csv')\n",
    "        self.den_del = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\deletion_denominator')\n",
    "        self.den_ins = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\insertion_denominator')\n",
    "        self.den_sub = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\substitution_denominator')\n",
    "        self.den_tra = joblib.load(r'C:\\Users\\User\\Desktop\\Software learning\\Assignment\\transposition_denominator')\n",
    "\n",
    "\n",
    "        self.weight = weight\n",
    "        self.threshold = threshold\n",
    "        self.true_prob = 0.0001\n",
    "\n",
    "        self.gui_layout()\n",
    "        print(\"\\nStatus: Ready\\n\")\n",
    "\n",
    "\n",
    "    def gui_layout(self):\n",
    "\n",
    "        window = tk.Tk()\n",
    "        window.configure(bg=\"Honeydew\")\n",
    "        window.geometry('{}x{}'.format(1530, 780))\n",
    "        window.title('Spelling Corrector')\n",
    "\n",
    "        window.grid_rowconfigure(1, weight=1)\n",
    "        window.grid_columnconfigure(0, weight=1)\n",
    "\n",
    "        # -------------------------------------------------------------------------------\n",
    "        # First container\n",
    "        self.top_frame = tk.Frame(master=window, bg=\"MediumSeaGreen\")\n",
    "        self.top_frame.pack(fill=tk.X)\n",
    "        logo = tk.Label(self.top_frame, text=\"English Checker\", font=(\"Arial Bold\", 18),\n",
    "                         fg='White', bg='MediumSeaGreen')\n",
    "        logo.grid(row=0, padx=20, ipady=10)\n",
    "\n",
    "        hr_space = tk.Frame(window, bg=\"Honeydew\", height=60)\n",
    "        hr_space.pack(fill=tk.X)\n",
    "\n",
    "        # -------------------------------------------------------------------------------\n",
    "        # Second container\n",
    "        # ****************************\n",
    "        # Create left frame\n",
    "        self.left_frame = tk.Frame(master=window, bg=\"light yellow\", width=800, height=60,\n",
    "                        borderwidth=2)\n",
    "        self.left_frame.pack(side=tk.LEFT, anchor=\"nw\", padx=40)\n",
    "\n",
    "        # Inside Left Frame - Header\n",
    "        ed_header = tk.Label(self.left_frame, text=\"Texts\", font=(\"Arial Bold\", 15),\n",
    "                                fg='black', bg='light yellow', pady=5)\n",
    "        ed_header.pack(padx=15, side=tk.TOP, anchor=\"nw\")\n",
    "\n",
    "        # Inside Left Frame - Text Box\n",
    "        self.txt = tk.Text(self.left_frame, width=100, height=20, bg=\"snow\",\n",
    "                     bd=1, relief=tk.GROOVE, padx=8, pady=5, font=(\"Arial\", 12))\n",
    "        self.txt.focus()\n",
    "        self.txt.pack(side=tk.TOP, anchor=\"nw\", padx=20, pady=10)\n",
    "\n",
    "        # Inside Left Frame - Check Button\n",
    "        self.check_button = tk.Button(self.left_frame, text=\"Check\", bg=\"Gainsboro\", fg=\"black\", \n",
    "                                      padx=10, command=lambda: [self.clear_output(), self.get_input()], pady=5)\n",
    "        self.check_button.pack(side=tk.TOP, anchor=\"nw\", padx=20, pady=(0, 10))\n",
    "    \n",
    "\n",
    "        # ****************************\n",
    "        # Create right frame\n",
    "        self.right_frame = tk.Frame(master=window, bg=\"light goldenrod yellow\", width=500, height=70)  # Snow\n",
    "        self.right_frame.pack(side=tk.RIGHT, anchor=\"ne\", padx=40)\n",
    "\n",
    "        # Inside Right Frame - Header\n",
    "        sg_header = tk.Label(self.right_frame, text=\"Suggestions: \", font=(\"Arial Bold\", 15),\n",
    "                                fg='black', bg='light goldenrod yellow', pady=5)\n",
    "        sg_header.pack(padx=15, side=tk.TOP, anchor=\"nw\")\n",
    "\n",
    "        # Inside Right Frame - Dictionary Box\n",
    "        self.lb = tk.Listbox(self.right_frame, bg=\"White\", fg=\"black\", width=60, \n",
    "                               relief=tk.GROOVE, selectmode=5, font=12, activestyle='none')\n",
    "        def selectedItem(event):\n",
    "            selected = self.lb.get(self.lb.curselection())\n",
    "            self.choose_correction(selected)\n",
    "            \n",
    "        self.lb.bind(\"<<ListboxSelect>>\", selectedItem)\n",
    "        self.lb.pack(padx=15, pady=10)\n",
    "\n",
    "        # Create search frame\n",
    "        self.search_frame = tk.Frame(master=self.right_frame, bg='light goldenrod yellow', width=50, height=35)\n",
    "        self.search_frame.pack(side=tk.TOP)\n",
    "\n",
    "        # Inside Search Frame - Search Box Header\n",
    "        self.search = tk.Button(self.search_frame, text=\"Search\", bg=\"Gainsboro\", padx=6.8, pady=5,\n",
    "                            fg=\"black\", relief=tk.GROOVE, command=self.Search)\n",
    "        self.search.pack(padx=5, pady=5, side=tk.LEFT)\n",
    "\n",
    "        # Inside Search Frame - Search Box\n",
    "        self.user_search = tk.StringVar()\n",
    "        self.search_box = tk.Entry(self.search_frame, text=\"testang\", bg=\"White\", fg=\"black\",\n",
    "                           font=11, textvariable=self.user_search)\n",
    "        self.search_box.pack(padx=5, pady=5, side=tk.RIGHT)\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------------\n",
    "        # Third container\n",
    "        left_space = tk.Frame(self.left_frame, bg=\"light yellow\", height=20)\n",
    "        left_space.pack(side=tk.TOP)\n",
    "        \n",
    "        # Clear button space\n",
    "        cb_space = tk.Frame(self.left_frame, bg=\"light yellow\", height=30, width=800)\n",
    "        cb_space.pack(side=tk.TOP)\n",
    "        \n",
    "        # Clear button\n",
    "        self.clear_button = tk.Button(master=cb_space, text=\"Clear\", bg=\"Gainsboro\", fg=\"black\", \n",
    "                                      command=self.clear_input, pady=5, width=6)\n",
    "        self.clear_button.pack(side=tk.TOP, pady=10)\n",
    "        \n",
    "        window.mainloop()\n",
    "\n",
    "        \n",
    "    def get_input(self, idx=\"1.0\"):\n",
    "        self.idx = idx\n",
    "        self.input = self.txt.get(self.idx, \"end-1c\")\n",
    "        self.spelling_correction()\n",
    "        \n",
    "    \n",
    "    def clear_output(self):\n",
    "        self.lb.delete(0, tk.END)\n",
    "    \n",
    "    \n",
    "    def clear_input(self):\n",
    "        self.txt.delete(\"1.0\", tk.END)\n",
    "        self.clear_output()\n",
    "        \n",
    "    def Search(self):\n",
    "        us = self.user_search.get()\n",
    "        for x in range(len(self.filtered_list)):\n",
    "            if us == self.filtered_list[x]:\n",
    "                self.lb.selection_set(x)\n",
    "        \n",
    "        \n",
    "    def choose_correction(self, selected):\n",
    "        self.lb.delete(0, tk.END)\n",
    "        self.txt.tag_remove(\"highlight\", \"1.0\", tk.END)\n",
    "        idx = self.txt.search(self.error[0], self.idx, nocase=1, stopindex=tk.END)\n",
    "        print(\"starting index: \", idx)\n",
    "        lastidx = '% s+% dc' % (idx, len(self.error[0]))\n",
    "        print(\"input last index: \", lastidx)\n",
    "        self.txt.delete(idx, lastidx)\n",
    "        self.txt.insert(idx, selected)\n",
    "        lastidx = '% s+% dc' % (idx, len(selected))\n",
    "        print(\"candidate last index:\", lastidx)\n",
    "        idx = lastidx\n",
    "        self.txt.tag_delete('highlight')\n",
    "        self.get_input(str(idx))\n",
    "        \n",
    "\n",
    "    def detect_name(self, token = None):\n",
    "        if (token in self.named_entity) | (self.input in self.named_entity):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def detect_dictionary(self, token = None):\n",
    "        if (token in self.dictionary) | (self.input in self.dictionary):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def segmenting_token(self, token = None):\n",
    "        candidates = []\n",
    "        bigram_model = self.kneser_bigram\n",
    "\n",
    "        for index in range(len(token)):\n",
    "            if token[index:] in self.wwn:\n",
    "                candidates.append([token[:index], token[index:]])\n",
    "            elif token[:index] in self.wwn:\n",
    "                candidates.append([token[:index], token[index:]])\n",
    "\n",
    "        if len(candidates) == 0:\n",
    "            output = [token]\n",
    "        elif len(candidates) == 1:\n",
    "            output = [word for sublist in candidates for word in sublist]\n",
    "        elif len(candidates) >= 2:\n",
    "            df = pd.DataFrame(data = candidates[0:], columns = [\"candidate1\", \"candidate2\"])\n",
    "            df[\"prob\"] = df.apply(lambda row: self.kneser_bigram.score(row[\"candidate2\"], row[\"candidate1\"].split()), axis = 1)\n",
    "            df[\"rank\"] = df[\"prob\"].rank(axis = 0, method = \"max\", ascending = False)\n",
    "            df[\"threshold\"] = np.where(df[\"prob\"] > self.threshold, 1, 0)\n",
    "            df.loc[(df[\"rank\"] == 1) & (df[\"threshold\"] == 1), \"selection\"] = 1\n",
    "            df_output = df.loc[df[\"selection\"] == 1, [\"candidate1\", \"candidate2\"]]\n",
    "\n",
    "            if df_output.shape[0] == 0:\n",
    "                output = [token]\n",
    "            else:\n",
    "                output = df_output.iloc[0].tolist()\n",
    "                output = [word for word in output if (word != \"\") | (word != \" \")]\n",
    "\n",
    "        return output # a list\n",
    "\n",
    "\n",
    "    def generation_confusion_sets(self, token = None): \n",
    "        index_1 = self.confusion_sets_1.loc[self.confusion_sets_1.apply(lambda row: row.str.fullmatch(token, case = False).any(), axis = 1)].index\n",
    "        output_confusion_sets_1 = self.confusion_sets_1.loc[self.confusion_sets_1.index.isin(index_1)].dropna(axis = 1).to_numpy().reshape(-1).tolist()\n",
    "\n",
    "        index_2 = self.confusion_sets_2[self.confusion_sets_2.apply(lambda row: row.str.fullmatch(token, case = False).any(), axis = 1)].index\n",
    "        output_confusion_sets_2 = self.confusion_sets_2.loc[self.confusion_sets_2.index.isin(index_2)].dropna(axis = 1).to_numpy().reshape(-1).tolist()\n",
    "\n",
    "        output_confusion_sets = list(set(output_confusion_sets_1 + output_confusion_sets_2))\n",
    "        return output_confusion_sets \n",
    "\n",
    "\n",
    "    def generation_edit_distance(self, token = None): \n",
    "\n",
    "        max_length_list = [len(token) - 1,  len(token), len(token) + 1]\n",
    "        df = pd.DataFrame(self.dictionary, columns = [\"original\"])\n",
    "        df[\"length\"] = df[\"original\"].apply(lambda x: len(x))\n",
    "        df = df.loc[df[\"length\"].isin(max_length_list)]\n",
    "\n",
    "        # generation\n",
    "        if self.distance_type == 1:  # Levenshtein (allow substitution)\n",
    "            df[\"edit_distance\"] = df[\"original\"].apply(lambda x: edit_distance(token, x))\n",
    "            df.sort_values(by = \"edit_distance\", ascending = True, inplace = True)\n",
    "\n",
    "        elif self.distance_type == 2: # Levenshtein (disallow substitution)\n",
    "            df[\"edit_distance\"] = df[\"original\"].apply(lambda x: edit_distance(token, x, substitution_cost = 2))\n",
    "            df.sort_values(by = \"edit_distance\", ascending = True, inplace = True)\n",
    "\n",
    "        elif self.distance_type == 3: # Damerau-Levenshtein \n",
    "            df[\"edit_distance\"] = df[\"original\"].apply(lambda x: edit_distance(token, x, transpositions = True))\n",
    "            df.sort_values(by = \"edit_distance\", ascending = True, inplace = True)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Distance type is out of option.\")\n",
    "\n",
    "        df_edit_1 = df.loc[df[\"edit_distance\"] <= 1]\n",
    "        df_edit_2 = df.loc[df[\"edit_distance\"] == 2]\n",
    "\n",
    "        df_edit_1 = df_edit_1[\"original\"].to_numpy().flatten().tolist()\n",
    "        df_edit_2 = df_edit_2[\"original\"].to_numpy().flatten().tolist()\n",
    "\n",
    "        return df_edit_1, df_edit_2 # output_edit_distance is a list \n",
    "\n",
    "\n",
    "    def language_model(self):\n",
    "        if (self.language_model_type == 1) & (self.n_gram == 2):\n",
    "            lm = self.laplace_bigram\n",
    "        elif (self.language_model_type == 1) & (self.n_gram == 3):\n",
    "            lm = self.laplace_trigram\n",
    "\n",
    "        elif (self.language_model_type == 2) & (self.n_gram == 2):\n",
    "            lm = self.absolute_bigram\n",
    "        elif (self.language_model_type == 2) & (self.n_gram == 3):\n",
    "            lm = self.absolute_trigram\n",
    "\n",
    "        elif (self.language_model_type == 3) & (self.n_gram == 2):\n",
    "            lm = self.kneser_bigram\n",
    "        elif (self.language_model_type == 3) & (self.n_gram == 3):\n",
    "            lm = self.kneser_trigram\n",
    "        else:\n",
    "            raise ValueError(\"Invalid type\")\n",
    "\n",
    "        return lm\n",
    "\n",
    "\n",
    "    def noisy_pattern(self, candidate, typed):\n",
    "\n",
    "        index_e = []\n",
    "        x = \"\"\n",
    "        y = \"\"\n",
    "        edit_type = \"\"\n",
    "\n",
    "        if len(candidate) == len(typed):\n",
    "            for i in range(len(candidate)):\n",
    "                if candidate[i] == typed[i]:\n",
    "                    pass\n",
    "                else:\n",
    "                    a = i\n",
    "                    index_e.append(a)\n",
    "\n",
    "        elif len(candidate) != len(typed):\n",
    "            if len(candidate) > len(typed):\n",
    "                edit_type = \"deletion\"\n",
    "                for i in range(len(candidate)):\n",
    "                    if i <= len(candidate) - 2:\n",
    "                        if candidate[i] == typed[i]:\n",
    "                                pass\n",
    "\n",
    "                        else:\n",
    "                            if i == 0:\n",
    "                                x = \"#\"\n",
    "                                y = candidate[i]\n",
    "                                break\n",
    "\n",
    "                            else:\n",
    "                                x = candidate[i-1]\n",
    "                                y = candidate[i]\n",
    "                                break\n",
    "\n",
    "                    elif i == len(candidate) - 1:\n",
    "                        x = candidate[i-1]\n",
    "                        y = candidate[i]\n",
    "                        break\n",
    "\n",
    "            elif len(candidate) < len(typed):\n",
    "                edit_type = \"insertion\"\n",
    "                for i in range(len(typed)):\n",
    "                    if i <= len(typed) - 2:\n",
    "                        if typed[i] == candidate[i]:\n",
    "                                pass\n",
    "                        else:\n",
    "                            if i == 0:\n",
    "                                x = \"#\"\n",
    "                                y = typed[i]\n",
    "                                break\n",
    "\n",
    "                            else:\n",
    "                                x = typed[i-1]\n",
    "                                y = typed[i] \n",
    "                                break\n",
    "\n",
    "                    elif i == len(typed) - 1:\n",
    "                        x = typed[i-1]\n",
    "                        y = typed[i]\n",
    "\n",
    "\n",
    "        if len(index_e) == 1: # must be substitution\n",
    "            edit_type = \"substitution\"\n",
    "            x = typed[index_e[0]]\n",
    "            y = candidate[index_e[0]]\n",
    "\n",
    "        elif len(index_e) == 2: # must be transposition\n",
    "            if (candidate[index_e[0]] == typed[index_e[1]]) & (candidate[index_e[1]] == typed[index_e[0]]):\n",
    "                edit_type = \"transposition\"  \n",
    "                x = candidate[index_e[0]]\n",
    "                y = candidate[index_e[1]]\n",
    "\n",
    "        return edit_type, x, y\n",
    "\n",
    "\n",
    "    def noisy_prob(self, candidate = None, edit_type = None, x = None, y = None):\n",
    "\n",
    "        numerator = None\n",
    "        denominator = None\n",
    "\n",
    "        num = None   # a dataframe for numerator\n",
    "        den = None  # for denominator\n",
    "\n",
    "        x_y = x + y\n",
    "\n",
    "        if edit_type == \"deletion\":\n",
    "            num = self.num_deletion\n",
    "            den = self.den_del\n",
    "\n",
    "            if num.loc[num[\"pattern\"] == x_y, \"count\"].shape[0] > 0:\n",
    "                numerator = num.loc[num[\"pattern\"] == x_y, \"count\"].iloc[0]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if x_y in den.keys():\n",
    "                denominator = den[x_y]\n",
    "            else: \n",
    "                pass\n",
    "\n",
    "        elif edit_type == \"insertion\":\n",
    "            num = self.num_insertion\n",
    "            den = self.den_ins\n",
    "\n",
    "            if num.loc[num[\"pattern\"] == x_y, \"count\"].shape[0] > 0:\n",
    "                numerator = num.loc[num[\"pattern\"] == x_y, \"count\"].iloc[0]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if x_y in den.keys():\n",
    "                denominator = den[x]\n",
    "            else: \n",
    "                pass\n",
    "\n",
    "        elif edit_type == \"substitution\":\n",
    "            num = self.num_substitution\n",
    "            den = self.den_sub\n",
    "\n",
    "            if num.loc[num[\"pattern\"] == x_y, \"count\"].shape[0] > 0:\n",
    "                numerator = num.loc[num[\"pattern\"] == x_y, \"count\"].iloc[0]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if x_y in den.keys():\n",
    "                denominator = den[y]\n",
    "            else: \n",
    "                pass\n",
    "\n",
    "        elif edit_type == \"transposition\":\n",
    "            num = self.num_transposition\n",
    "            den = self.den_tra\n",
    "\n",
    "            if num.loc[num[\"pattern\"] == x_y, \"count\"].shape[0] > 0:\n",
    "                numerator = num.loc[num[\"pattern\"] == x_y, \"count\"].iloc[0]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if x_y in den.keys():\n",
    "                denominator = den[x_y]\n",
    "            else: \n",
    "                pass\n",
    "\n",
    "        # replace with zero in order to avoid a zero division error\n",
    "        if (denominator == None) | (numerator == None):\n",
    "            noisy_prob = 0\n",
    "        else:\n",
    "            noisy_prob = numerator / denominator \n",
    "\n",
    "        return noisy_prob\n",
    "\n",
    "\n",
    "    def noisy_prob_compute(self, candidate_1, token):\n",
    "\n",
    "        df_edit_1 = pd.DataFrame(data = list(zip(candidate_1, [\"edit_1\"] * len(candidate_1))), columns = [\"candidates\", \"edit_distance\"])\n",
    "        df_edit_1[\"edit_type\"] = df_edit_1[\"candidates\"].apply(lambda row: self.noisy_pattern(row, token))\n",
    "\n",
    "        df_edit_1[\"x\"] = df_edit_1[\"edit_type\"].apply(lambda row: row[1]) \n",
    "        df_edit_1[\"y\"] = df_edit_1[\"edit_type\"].apply(lambda row: row[2]) \n",
    "        df_edit_1[\"edit_type\"] = df_edit_1[\"edit_type\"].apply(lambda row: row[0]) \n",
    "\n",
    "        df_edit_1[\"noisy_prob\"] = df_edit_1.apply(lambda row: self.noisy_prob(candidate = row[\"candidates\"], \n",
    "                                                                            edit_type = row[\"edit_type\"], \n",
    "                                                                            x = row[\"x\"], y = row[\"y\"]), axis = 1)\n",
    "        return df_edit_1\n",
    "\n",
    "\n",
    "    def lm_prob_compute(self, df_edit, token_i1, token_i2, index, token_length):\n",
    "        lm = self.language_model()\n",
    "        \n",
    "        if (token_length==1):\n",
    "            df_edit[\"prior_prob\"] = df_edit[\"candidates\"].apply(lambda row:lm.score(row))\n",
    "        else:\n",
    "            if (index == 0) & (self.n_gram == 2):\n",
    "                df_edit[\"prior_prob\"] = df_edit[\"candidates\"].apply(lambda row: lm.score(row, \"<s>\".split()))\n",
    "            elif (index == 0) & (self.n_gram == 3):\n",
    "                df_edit[\"prior_prob\"] = df_edit[\"candidates\"].apply(lambda row: lm.score(row, \"<s> <s>\".split()))\n",
    "            elif (index != 0) & (self.n_gram == 2):\n",
    "                df_edit[\"prior_prob\"] = df_edit[\"candidates\"].apply(lambda row: lm.score(row, token_i1.split()))\n",
    "            elif (index == 1) & (self.n_gram == 3):\n",
    "                df_edit[\"prior_prob\"] = df_edit[\"candidates\"].apply(lambda row: lm.score(row, \" \".join(map(str, [\"<s>\", token_i1])).split()))\n",
    "            elif (index != 0) & (self.n_gram == 3):\n",
    "                df_edit[\"prior_prob\"] = df_edit[\"candidates\"].apply(lambda row: lm.score(row, \" \".join(map(str, [token_i2, token_i1])).split()))\n",
    "            else:\n",
    "                raise ValueError(\"Invalid\")\n",
    "\n",
    "        return df_edit\n",
    "\n",
    "\n",
    "    def print_candidate_words(self, candidate_list = None):\n",
    "        self.filtered_list = candidate_list\n",
    "        for x in range(1, len(self.filtered_list) + 1, 1):\n",
    "            self.lb.insert(x-1, self.filtered_list[x-1])\n",
    "        \n",
    "        return self.filtered_list\n",
    "\n",
    "\n",
    "    def spelling_correction(self):\n",
    "        print(\"Running spelling_correction\")\n",
    "        tokens = []\n",
    "        self.error = []\n",
    "\n",
    "        for word in word_tokenize(self.input):\n",
    "            if word.lower() in self.named_entity:\n",
    "                tokens.append(word)\n",
    "            else:\n",
    "                tokens.append(word.lower()) \n",
    "\n",
    "        if len(tokens) > 500:\n",
    "            tk.messagebox.showerror(\"Warning\", \"You have exceeded the 500 words limit! Please adjust your word counts.\")\n",
    "        else:\n",
    "\n",
    "          for i in range(len(tokens)):\n",
    "              x = self.segmenting_token(token = tokens[i])\n",
    "              tokens[i] = x                          \n",
    "\n",
    "          tokens = [[word] for sublist in tokens for word in sublist if word != \"\"]\n",
    "          print(\"Finished Tokenization\")\n",
    "\n",
    "          for i in range(len(tokens)):\n",
    "              print(\"Checking the existence of token in name\")\n",
    "              # Step 1: check if it is a name\n",
    "              if self.detect_name(token = tokens[i][0].lower()) is True:\n",
    "                  print(\"Token is found in NAME\")\n",
    "                  pass\n",
    "              else:\n",
    "\n",
    "                  # Step 1.1: check if it is a real word\n",
    "                  print(\"Checking dictionary\")\n",
    "                  if self.detect_dictionary(token = tokens[i][0]) is True:\n",
    "                      \n",
    "                      print(\"Word is found in dictionary, proceed with real word detection\")\n",
    "                      # generate candidates\n",
    "                      con_set = self.generation_confusion_sets(token = tokens[i][0])\n",
    "                      candidate_1, candidate_2 = self.generation_edit_distance(token = tokens[i][0])\n",
    "                      \n",
    "                      con_set = [word for word in con_set if (word not in candidate_1) & (word not in candidate_2)]\n",
    "                      candidate_1 = [word for word in candidate_1 if (word not in candidate_2) & (word not in con_set)]\n",
    "                      candidate_2 = [word for word in candidate_2 if (word not in candidate_1) & (word not in con_set)]\n",
    "\n",
    "\n",
    "                      if len(candidate_1) < 5:\n",
    "                          df_edit_2 = pd.DataFrame(data = list(zip(candidate_2, [\"edit_2\"] * len(candidate_2))), columns = [\"candidates\", \"edit_distance\"])\n",
    "                          df_edit_2[\"jaro_score\"] = df_edit_2[\"candidates\"].apply(lambda row: jaro.jaro_winkler_metric(tokens[i][0], row))\n",
    "                          df_edit_2 = df_edit_2.sort_values(by = \"jaro_score\", ascending = False)\n",
    "                          df_edit_2.reset_index(drop = True, inplace = True)\n",
    "                          df_edit_2 = df_edit_2.iloc[0:5 - len(candidate_1)]\n",
    "                          if (len(candidate_1) == 0) & (len(con_set) == 0):\n",
    "                                  df_edit = df_edit_2.copy()\n",
    "                          elif (len(candidate_1) != 0) & (len(con_set) == 0):\n",
    "                                  df_edit_1 = self.noisy_prob_compute(candidate_1 = candidate_1, token = tokens[i][0])\n",
    "                                  df_edit = pd.concat([df_edit_1, df_edit_2])\n",
    "                          elif (len(candidate_1) != 0) & (len(con_set) != 0):\n",
    "                                  df_edit_1 = self.noisy_prob_compute(candidate_1 = candidate_1, token = tokens[i][0])\n",
    "                                  df_con_set = pd.DataFrame(data = list(zip(con_set, [\"con_set\"] * len(con_set))), columns = [\"candidates\", \"edit_distance\"])\n",
    "                                  df_edit = pd.concat([df_edit_1, df_edit_2, df_con_set])\n",
    "                          elif (len(candidate_1) == 0) & (len(con_set) != 0):\n",
    "                                  df_con_set = pd.DataFrame(data = list(zip(con_set, [\"con_set\"] * len(con_set))), columns = [\"candidates\", \"edit_distance\"])\n",
    "                                  df_edit = pd.concat([df_edit_2, df_con_set])\n",
    "                      else:\n",
    "                          df_edit_1 = self.noisy_prob_compute(candidate_1 = candidate_1, token = tokens[i][0])\n",
    "                          if len(con_set) == 0:\n",
    "                              df_edit = df_edit_1.copy()\n",
    "                          else:\n",
    "                              df_con_set = pd.DataFrame(data = list(zip(con_set, [\"con_set\"] * len(con_set))), columns = [\"candidates\", \"edit_distance\"])\n",
    "                              df_edit = pd.concat([df_edit_1, df_con_set])\n",
    "\n",
    "                      # other controls are done in lm_prob_compute\n",
    "                      if (i != 0) & (self.n_gram == 2):\n",
    "                          token_i1 = tokens[i-1][0]\n",
    "                          token_i2 = None\n",
    "                      elif (i == 1) & (self.n_gram == 3):\n",
    "                          token_i1 = tokens[i-1][0]\n",
    "                          token_i2 = None\n",
    "                      elif (i != 0) & (self.n_gram == 3):\n",
    "                          token_i1 = tokens[i-1][0]\n",
    "                          token_i2 = tokens[i-2][0]\n",
    "                      else:\n",
    "                          token_i1 = None\n",
    "                          token_i2 = None                           \n",
    "                              \n",
    "                      df_edit = self.lm_prob_compute(df_edit = df_edit, token_i1 = token_i1, token_i2 = token_i2, index = i, token_length = len(tokens))\n",
    "\n",
    "                      df_edit[\"sum_prob\"] = df_edit.apply(lambda row: (row[\"prior_prob\"] * self.weight) + ((row[\"noisy_prob\"]) * (1 - self.weight)) if row[\"edit_distance\"] == \"edit_1\" else\n",
    "                                          row[\"prior_prob\"], axis = 1)\n",
    "                      df_edit = df_edit.sort_values(by = \"sum_prob\", ascending = False)\n",
    "\n",
    "                      total_prob = df_edit[\"sum_prob\"].sum()\n",
    "                      df_edit[\"normalise_prob\"] = df_edit[\"sum_prob\"].apply(lambda row: row / total_prob)\n",
    "\n",
    "                      prob = df_edit.loc[df_edit[\"candidates\"] == tokens[i][0], \"normalise_prob\"].iloc[0]\n",
    "                      if prob > self.true_prob:\n",
    "                          print(\"Token is correct!\")\n",
    "                          pass\n",
    "                      else:\n",
    "                          print(\"The word has real word error\")\n",
    "                          self.error.append(tokens[i][0]) \n",
    "                          for err in self.error:\n",
    "                              idx = self.txt.search(err, self.idx, nocase=1, stopindex=tk.END)\n",
    "                              lastidx = '%s+%dc' % (idx, len(err))\n",
    "                              self.txt.tag_add(\"highlight\", idx, lastidx)\n",
    "                              #idx = lastidx\n",
    "                              self.txt.tag_config(\"highlight\", background=\"yellow\", foreground=\"red\")\n",
    "                          if df_edit.shape[0] > 5:\n",
    "                              candidates = df_edit[\"candidates\"].to_numpy().flatten().tolist()[:5]\n",
    "                              show_candidate = self.print_candidate_words(candidate_list = candidates)\n",
    "                              break\n",
    "                          else:\n",
    "                              candidates = df_edit[\"candidates\"].to_numpy().flatten().tolist()\n",
    "                              show_candidate = self.print_candidate_words(candidate_list = candidates)\n",
    "                              break\n",
    "\n",
    "\n",
    "                  # Step 1.2, for non-word errors, to calculate edit distance\n",
    "                  else: \n",
    "                      print(\"The word has non word error\")\n",
    "                      self.error.append(tokens[i][0])\n",
    "                      for err in self.error:\n",
    "                          idx = self.txt.search(err, self.idx, nocase=1, stopindex=tk.END)\n",
    "                          lastidx = '%s+%dc' % (idx, len(err))\n",
    "                          self.txt.tag_add(\"highlight\", idx, lastidx)\n",
    "                          #idx = lastidx\n",
    "                          self.txt.tag_config(\"highlight\", background=\"yellow\", foreground=\"red\")\n",
    "                      candidates_1, candidates_2 = self.generation_edit_distance(token = tokens[i][0])\n",
    "                      df_edit_1 = self.noisy_prob_compute(candidate_1 = candidates_1, token = tokens[i][0])\n",
    "\n",
    "                      if df_edit_1.shape[0] >= 5:\n",
    "                          df_edit = df_edit_1\n",
    "                      else:\n",
    "                          df_edit_2 = pd.DataFrame(data = list(zip(candidates_2, [\"edit_2\"] * len(candidates_2))), columns = [\"candidates\", \"edit_distance\"])\n",
    "                          df_edit_2[\"jaro_score\"] = df_edit_2[\"candidates\"].apply(lambda row: jaro.jaro_winkler_metric(tokens[i][0], row))\n",
    "                          df_edit_2 = df_edit_2.sort_values(by = \"jaro_score\", ascending = False)\n",
    "                          df_edit_2.reset_index(drop = True, inplace = True)\n",
    "                          df_edit_2 = df_edit_2.iloc[0:5 - df_edit_1.shape[0]]\n",
    "                          df_edit = pd.concat([df_edit_1, df_edit_2])\n",
    "\n",
    "                      if df_edit.shape[0] == 0:\n",
    "                          candidates = []\n",
    "                      else:\n",
    "                          # other controls are done in lm_prob_compute\n",
    "                          if (i != 0) & (self.n_gram == 2):\n",
    "                              token_i1 = tokens[i-1][0]\n",
    "                              token_i2 = None\n",
    "                          elif (i == 1) & (self.n_gram == 3):\n",
    "                              token_i1 = tokens[i-1][0]\n",
    "                              token_i2 = None\n",
    "                          elif (i != 0) & (self.n_gram == 3):\n",
    "                              token_i1 = tokens[i-1][0]\n",
    "                              token_i2 = tokens[i-2][0]\n",
    "                          else:\n",
    "                              token_i1 = None\n",
    "                              token_i2 = None \n",
    "                              \n",
    "                          df_edit = self.lm_prob_compute(df_edit = df_edit, token_i1 = token_i1, token_i2 = token_i2, index = i, token_length = len(tokens))\n",
    "\n",
    "                          df_edit[\"sum_prob\"] = df_edit.apply(lambda row: (row[\"prior_prob\"] * self.weight) + ((row[\"noisy_prob\"]) * (1 - self.weight)) if row[\"edit_distance\"] == \"edit_1\" else\n",
    "                                                      row[\"prior_prob\"], axis = 1)\n",
    "\n",
    "                          df_edit = df_edit.sort_values(by = \"sum_prob\", ascending = False)\n",
    "\n",
    "                          if df_edit.shape[0] > 5:\n",
    "                              candidates = df_edit[\"candidates\"].to_numpy().flatten().tolist()[:5]\n",
    "                              show_candidate = self.print_candidate_words(candidate_list = candidates)\n",
    "                              break\n",
    "                          else:\n",
    "                              candidates = df_edit[\"candidates\"].to_numpy().flatten().tolist()\n",
    "                              show_candidate = self.print_candidate_words(candidate_list = candidates)\n",
    "                              break\n",
    "\n",
    "\n",
    "    def tuning(self, token_t, token_t_1, token_t_2, index):\n",
    "\n",
    "          candidates_1, candidates_2 = self.generation_edit_distance(token = token_t)\n",
    "\n",
    "          if len(candidates_1) == 0:\n",
    "              df_edit = pd.DataFrame(data = list(zip(candidates_2, [\"edit_2\"] * len(candidates_2))), columns = [\"candidates\", \"edit_distance\"])\n",
    "          elif len(candidates_1) >= 5:\n",
    "              df_edit = self.noisy_prob_compute(candidate_1 = candidates_1, token = token_t)\n",
    "          else:\n",
    "              df_edit_2 = pd.DataFrame(data = list(zip(candidates_2, [\"edit_2\"] * len(candidates_2))), columns = [\"candidates\", \"edit_distance\"])\n",
    "              df_edit_2[\"jaro_score\"] = df_edit_2[\"candidates\"].apply(lambda row: jaro.jaro_winkler_metric(token_t, row))\n",
    "              df_edit_2 = df_edit_2.sort_values(by = \"jaro_score\", ascending = False)\n",
    "              df_edit_2.reset_index(drop = True, inplace = True)\n",
    "              df_edit_1 = self.noisy_prob_compute(candidate_1 = candidates_1, token = token_t)\n",
    "              df_edit_2 = df_edit_2.iloc[0:5 - df_edit_1.shape[0]]\n",
    "              df_edit = pd.concat([df_edit_1, df_edit_2])\n",
    "\n",
    "          df_edit = self.lm_prob_compute(df_edit = df_edit, token_i1 = token_t_1, token_i2 = token_t_2, index = index)\n",
    "\n",
    "          if df_edit.shape[0] == 0:\n",
    "              candidates = []\n",
    "          else:\n",
    "              df_edit[\"sum_prob\"] = df_edit.apply(lambda row: (row[\"prior_prob\"] * self.weight) + ((row[\"noisy_prob\"]) * (1 - self.weight)) if row[\"edit_distance\"] == \"edit_1\" else\n",
    "                                              row[\"prior_prob\"], axis = 1)\n",
    "\n",
    "              df_edit = df_edit.sort_values(by = \"sum_prob\", ascending = False)\n",
    "\n",
    "              if df_edit.shape[0] > 5:\n",
    "                  candidates = df_edit[\"candidates\"].to_numpy().flatten().tolist()[:5]\n",
    "              else:\n",
    "                  candidates = df_edit[\"candidates\"].to_numpy().flatten().tolist()\n",
    "\n",
    "          return candidates        \n",
    "          \n",
    "          \n",
    "if __name__ == \"__main__\":\n",
    "  gui = spelling_correction(distance_type = 3, language_model_type = 3, ngram = 3, weight = 0.8, threshold = 1.e-02).spelling_correction()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1a9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
